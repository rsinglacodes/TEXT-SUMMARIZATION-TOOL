{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79cfc654-5fa9-4e6c-95f0-1b77aa634b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfd8d49-f99b-4292-8137-1791692240dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\singl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "448a2ab0-46ad-4ee8-971e-787e92b4b3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\singl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d697ed9-246f-4c7a-8e02-915a08d2e580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\singl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4d8e936-1e36-493c-8f3b-0845e2c65e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_sentence(sent: str) -> str:\n",
    "    \"\"\"Cleans and tokenizes a sentence for TF-IDF processing.\"\"\"\n",
    "    toks = [w.lower() for w in word_tokenize(sent) if any(c.isalnum() for c in w)]\n",
    "    toks = [w for w in toks if w not in STOPWORDS]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def extractive_summary(text: str, num_sentences: int = 3) -> str:\n",
    "    \"\"\"Extractive summary using TF-IDF sentence ranking.\"\"\"\n",
    "    sentences = sent_tokenize(text.strip())\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text.strip()\n",
    "\n",
    "    cleaned = [clean_sentence(s) for s in sentences]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(cleaned)\n",
    "    scores = np.array(tfidf.sum(axis=1)).ravel()\n",
    "    top_idx = scores.argsort()[-num_sentences:][::-1]\n",
    "    top_idx_sorted = sorted(top_idx)\n",
    "    summary = \" \".join([sentences[i] for i in top_idx_sorted])\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffa6687d-1389-4254-9a84-fb134f29806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstractive_pipeline(model_name: str = \"sshleifer/distilbart-cnn-12-6\"):\n",
    "    \"\"\"Creates and returns a summarization pipeline.\"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=model_name)\n",
    "    return summarizer\n",
    "\n",
    "def chunk_text(text: str, max_chars: int = 1000):\n",
    "    \"\"\"Splits text into chunks for large documents.\"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    chunks = []\n",
    "    for p in paragraphs:\n",
    "        if len(p) <= max_chars:\n",
    "            chunks.append(p)\n",
    "        else:\n",
    "            sents = sent_tokenize(p)\n",
    "            current = \"\"\n",
    "            for s in sents:\n",
    "                if len(current) + len(s) + 1 <= max_chars:\n",
    "                    current += \" \" + s\n",
    "                else:\n",
    "                    chunks.append(current.strip())\n",
    "                    current = s\n",
    "            if current.strip():\n",
    "                chunks.append(current.strip())\n",
    "    if not chunks:\n",
    "        for i in range(0, len(text), max_chars):\n",
    "            chunks.append(text[i:i+max_chars])\n",
    "    return chunks\n",
    "\n",
    "def abstractive_summary(text: str, summarizer, max_chunk_chars: int = 1000, ratio: float = 0.2) -> str:\n",
    "    \"\"\"Performs abstractive summarization using a transformer model.\"\"\"\n",
    "    chunks = chunk_text(text, max_chars=max_chunk_chars)\n",
    "    chunk_summaries = []\n",
    "    for chunk in chunks:\n",
    "        max_len = max(50, min(200, int(len(chunk) * ratio / 4)))\n",
    "        min_len = max(10, int(max_len * 0.3))\n",
    "        out = summarizer(chunk, max_length=max_len, min_length=min_len, do_sample=False)\n",
    "        chunk_summaries.append(out[0]['summary_text'].strip())\n",
    "\n",
    "    if len(chunk_summaries) == 1:\n",
    "        return chunk_summaries[0]\n",
    "\n",
    "    concat = \" \".join(chunk_summaries)\n",
    "    final_out = summarizer(concat, max_length=150, min_length=40, do_sample=False)\n",
    "    return final_out[0]['summary_text'].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2adc2d11-f37f-4017-a501-c9fcc18e488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Original Text:\n",
      "\n",
      "Paste your own article text here...\n",
      "\n",
      "\n",
      "ðŸ”¹ Extractive Summary:\n",
      "Paste your own article text here...\n"
     ]
    }
   ],
   "source": [
    "example_text = \"\"\"\n",
    "Paste your own article text here...\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"ðŸ”¹ Original Text:\")\n",
    "print(example_text)\n",
    "print(\"\\nðŸ”¹ Extractive Summary:\")\n",
    "print(extractive_summary(example_text, num_sentences=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1445a5-63c8-45e4-8b6d-b360a6637245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
